---
title: 'R, Rcpp and Parallel Computing'
subtitle: 'Notes from our Rcpp Experience'
author: "Dirk Eddelbuettel and JJ Allaire"
date: "Jan 26-27, 2015\\newline Workshop for Distributed Computing in R"
output:
  beamer_presentation:
    incremental: no
    keep_tex: yes
    theme: Warsaw
    includes:
      in_header: header.tex
fontsize: 12pt
classoption: compress
---

# Intro

## One View on Parallel Computing

> The whole "let's parallelize" thing is a huge waste of everybody's
> time. There's this huge body of "knowledge" that parallel is somehow more
> efficient, and that whole huge body is pure and utter garbage. Big caches
> are efficient. Parallel stupid small cores without caches are horrible
> unless you have a very specific load that is hugely regular (ie graphics). 
>   
> [...]  
>   
> Give it up. The whole "parallel computing is the future" is a bunch of crock.


[Linus Torvalds, Dec 2014](http://www.realworldtech.com/forum/?threadid=146066&curpostid=146227)


## Another View on Big Data

\begin{figure}
\begin{center}
\includegraphics[width=\textwidth,height=0.8\textheight,keepaspectratio]{images/big-data-big-machine-tweet.png}
\end{center}
\end{figure}

# R

## CRAN Task View on HPC

\framesubtitle{Lots of existing work to draw from}

- Package snow by Tierney et al a trailblazer
- Package Rmpi equally important
- Packages multicore nee parallel help Windows (l)users
- Hundreds of applications
- It just works for _data parallel_ tasks

<http://cran.r-project.org/web/views/HighPerformanceComputing.html>

## Rcpp: Early Days

In the fairly early days of Rcpp, we also put out RInside as a simple C++
class wrapper around the R-embedding API.

It got one clever patch taking this (ie: R wrapped in C++ with its own
`main()` and sticking it into MPI. 

## Rcpp: More recently

Rcpp has gotten fairly easy to use.  

OpenMP is easy to use and widely
supported (on suitable OS / compiler combinations).

So we added support.  Use not as wide-spread. Errors have commonality:
calling back into R.


## Parallel Programming for Rcpp Users

NOT like this...

```cpp
using namespace boost;

void task()
{
   lock_guard<boost::mutex> lock(mutex);
   // etc...
}

threadpool::pool tp(thread::hardware_concurrency());
for (int i=0; i<slices; i++)
   tp.schedule(&task); 
```

## Parallel Programming for Rcpp Users

Goals:

* Encapsulate threading and locking
* Provide high level constructs for common parallel tasks
* High performance: locality, work stealing, etc.
* Safe access to R data structures on multiple threads


## Parallel Programming Alternatives
   
|   | TBB | OMP | RAW |
|---|:----------:|:------:|:-------:|
Task level parallelism | \textbullet | \textbullet |   |
Data decomposition support | \textbullet | \textbullet |   |
Non loop parallel patterns | \textbullet |   |   |
Generic parallel patterns | \textbullet |   |   |
Nested parallelism support | \textbullet |   |   |
Built in load balancing | \textbullet | \textbullet |   |
Affinity support |   | \textbullet | \textbullet |
Static scheduling |   | \textbullet |   |
Concurrent data structures | \textbullet |   |   |
Scalable memory allocator | \textbullet |   |   |
I/O dominated tasks |   |   | \textbullet |
User synchronization primitives | \textbullet | \textbullet |   |

## TBB vs. OpenMP vs. Threads

* Raw threads shift too much burden for parallelization onto the developer (error prone and not performant)
* OpenMP is excellent for parallelizing existing loops where the iterations are independent (R already has some support for OpenMP)
* TBB fares better when there is more potential interaction between threads (e.g. more complex loops, simulations, or where concurrent containers are required).
* RcppParallel: Enable use of TBB with R to compliment existing OpenMP stack.










